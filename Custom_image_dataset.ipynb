{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Custom_image_dataset.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"d946fb22"},"source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from torchvision.transforms import ToTensor, Lambda, Normalize, CenterCrop, Resize\n","from torchvision.io.image import ImageReadMode\n","\n","import cv2\n","import numpy as np\n","\n","\n","\n","class CustomImageDataset(Dataset):\n","    \n","    def __init__(self, files, labels):\n","        \"files: contains the list of path to each file\"\n","        \"labels: a list of all the labels in order of the image paths in files\"\n","        self.files = files\n","        self.labels = labels\n","        self.transform = Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","        self.target_transform = Lambda(lambda y: torch.zeros(3, \n","                                        dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n","        \n","    def __len__(self):\n","        \"denotes the total number of samples\"\n","        return len(self.files)\n","    \n","    def __getitem__(self, index):\n","        \"read one sample\"\n","        x = read_image(self.files[index])\n","        y = self.labels[index]\n","\n","        x = Resize((400, 800))(x)\n","        x = self.transform(x.type(torch.float))\n","        x = x.type(torch.float)\n","        y = self.target_transform(y)\n","        \n","        return x, y"],"id":"d946fb22","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"84db4e5e"},"source":[""],"id":"84db4e5e","execution_count":null,"outputs":[]}]}